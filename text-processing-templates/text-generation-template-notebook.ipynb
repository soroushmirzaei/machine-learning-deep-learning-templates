{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soroushmirzaei/projects-notebook-templates/blob/main/text-processing-templates/text-generation-template-notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIYC55FdPdC2"
      },
      "outputs": [],
      "source": [
        "#import requirement libraries\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "#import dataset query libraries\n",
        "import csv\n",
        "import json\n",
        "\n",
        "#import mathematics statics libraries\n",
        "import random as rnd\n",
        "import numpy as np\n",
        "\n",
        "#import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#import machine learning deep learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SkBIh-h6ZzE"
      },
      "outputs": [],
      "source": [
        "#download filters-characters dataset\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/english-language-filter-characters.txt\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/persian-language-filter-characters.txt\n",
        "\n",
        "#download similar-characters dataset\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/persian-language-similar-characters.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cna6blsV2ZkZ"
      },
      "outputs": [],
      "source": [
        "#define filters-list function loader\n",
        "def filter_chars(file_path):\n",
        "    filter_chars = list()\n",
        "    with open(file_path, 'r') as filters_list_file:\n",
        "        for word in filters_list_file:\n",
        "            filter_chars.append(word.strip('\\n'))\n",
        "        filters_list_file.close()\n",
        "    return filter_chars\n",
        "\n",
        "#define similar-characters function loader\n",
        "def similar_chars(file_path):\n",
        "    with open(file_path, 'r') as similar_chars_file:\n",
        "        similar_chars = json.load(similar_chars_file)\n",
        "    return similar_chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLhC44LW6TAd"
      },
      "outputs": [],
      "source": [
        "#load filters-characters\n",
        "eng_filter_characters = filter_chars('english-language-filter-characters.txt')\n",
        "per_filter_characters = filter_chars('persian-language-filter-characters.txt')\n",
        "\n",
        "#load similar-characters\n",
        "per_similar_characters = similar_chars('persian-language-similar-characters.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39pSrmpdM5xg"
      },
      "outputs": [],
      "source": [
        "#define remove filters characters function\n",
        "def remove_filter(text, filters_list):\n",
        "    characters = list(text)\n",
        "    characters_without_filters = [character for character in characters if character not in filters_list]\n",
        "    text_without_filters = ''.join(characters_without_filters)\n",
        "    return text_without_filters\n",
        "\n",
        "#define similar characters modification function\n",
        "def similar_char(text, similar_chars_dict):\n",
        "    characters = list(text)\n",
        "    similar_characters_modified_list = [similar_chars_dict.get(character,character) for character in characters]\n",
        "    similar_characters_modified_text = ''.join(similar_characters_modified_list)\n",
        "    return similar_characters_modified_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljecAnF0V6C9"
      },
      "outputs": [],
      "source": [
        "#define tensorflow datasets texts labels loader\n",
        "def tfds_text(#define dataset\n",
        "              data_set,\n",
        "              #define preprocessing function for texts\n",
        "              use_filter_remover = False, filters_list = None\n",
        "              ):\n",
        "    \n",
        "    #create empty texts labels list\n",
        "    texts_list = list()\n",
        "\n",
        "    for text in data_set['train']:\n",
        "        text = text.numpy().decode('utf8')\n",
        "        #optional modification function\n",
        "        if use_filter_remover:\n",
        "            text = remove_filter(text, filters_list)\n",
        "        texts_list.append(text)\n",
        "\n",
        "    return texts_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs1sANpkK_xq"
      },
      "outputs": [],
      "source": [
        "#define texts and labels list loader for csv and json files\n",
        "def texts_loader(#define file path and type\n",
        "                 file_path, file_type,\n",
        "                 #define csv and txt files index for text and labels\n",
        "                 text_index = None, header_row = True, spliter_delimiter = None,\n",
        "                 #define json file keys for texts and labels\n",
        "                 text_key = None,\n",
        "                 #define preprocessing function for texts\n",
        "                 use_filter_remover = False, filters_list = None,\n",
        "                 use_similarchars_modifier = False, similarchars_dict = None\n",
        "                 ):\n",
        "    \n",
        "    #create empty texts labels list\n",
        "    texts_list = list()\n",
        "\n",
        "    #csv file loader\n",
        "    if file_type in ['csv']:\n",
        "        with open(file_path, 'r') as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter = spliter_delimiter)\n",
        "            if header_row:\n",
        "                next(csv_reader)\n",
        "            for row in csv_reader:\n",
        "                text = row[text_index]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                texts_list.append(text)\n",
        "        csv_file.close()\n",
        "\n",
        "    #txt file loader\n",
        "    if file_type in ['txt']:\n",
        "        with open(file_path, 'r') as txt_file:\n",
        "            for line in txt_file:\n",
        "                line = line.split(spliter_delimiter)\n",
        "                text = line[text_index]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                texts_list.append(text.strip('\\n'))\n",
        "        txt_file.close()\n",
        "    \n",
        "    #json file loader\n",
        "    if file_type in ['json']:\n",
        "        with open(file_path, 'r') as json_file:\n",
        "            json_reader = json.load(json_file)\n",
        "            for item in json_reader:\n",
        "                text = item[text_key]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                texts_list.append(text)\n",
        "        json_file.close()\n",
        "\n",
        "    return texts_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xws6QVTWPwbW"
      },
      "outputs": [],
      "source": [
        "#define tokenizer and sequences and padding sequences\n",
        "def texts_labels_generator(#define texts\n",
        "                           texts_list,\n",
        "                           #define filter characters list\n",
        "                           use_modified_filters = False, filters_list = None,\n",
        "                           #define json tokenizer\n",
        "                           save_tokenizer_json = False, tokenizer_filepath = None\n",
        "                           ):\n",
        "    \n",
        "    #define tokenizer filters and fit on texts\n",
        "    from keras.preprocessing.text import Tokenizer\n",
        "    if use_modified_filters:\n",
        "        filters = ''.join(filters_list)\n",
        "    else:\n",
        "        filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "    tokenizer = Tokenizer(filters = filters)\n",
        "    tokenizer.fit_on_texts(texts_list)\n",
        "\n",
        "    #define word_index\n",
        "    word_index = tokenizer.word_index\n",
        "    #for padding and counting out of vocab word\n",
        "    total_words = len(word_index) + 1\n",
        "\n",
        "    #save tokenizer json file\n",
        "    if save_tokenizer_json:\n",
        "        with open(tokenizer_filepath+'.json','w') as tokenizer_file:\n",
        "            json.dump(tokenizer.to_json(), tokenizer_file)\n",
        "\n",
        "    #define texts to sequences\n",
        "    texts_sequences = tokenizer.texts_to_sequences(texts_list)\n",
        "\n",
        "    #define phrase based sequences\n",
        "    phrases_sequences = list()\n",
        "\n",
        "    for text_sequence in texts_sequences:\n",
        "        for token_iter in range(1, len(text_sequence)):\n",
        "            phrase_sequence = text_sequence[:token_iter+1]\n",
        "            phrases_sequences.append(phrase_sequence)\n",
        "\n",
        "    #define maximum length of the sequences\n",
        "    maxlen = max([len(sequence) for sequence in phrases_sequences])\n",
        "\n",
        "    #define training validation pad sequences\n",
        "    from keras.preprocessing.sequence import pad_sequences\n",
        "    padded_sequences = pad_sequences(phrases_sequences, maxlen = maxlen, padding = 'pre')\n",
        "\n",
        "    #split texts and labels\n",
        "    texts = padded_sequences[:,:-1]\n",
        "    labels = padded_sequences[:,-1]\n",
        "\n",
        "    return texts, labels, maxlen, tokenizer, word_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ3p8PBDbFH2"
      },
      "outputs": [],
      "source": [
        "#define labels encoder\n",
        "def label_encoder(#define labels list and method\n",
        "                  labels_list,\n",
        "                  #define method binary, ordinal or onehot\n",
        "                  method, return_categories = True\n",
        "                  ):\n",
        "    \n",
        "    #ordinal and binary encoder method\n",
        "    if method in ['binary','ordinal']:\n",
        "        unique_labels = sorted(list(set(labels_list)))\n",
        "        labels_dict = {\n",
        "            label : int(unique_labels.index(label)) for label in unique_labels\n",
        "        }\n",
        "        labels = list(map(lambda label : labels_dict[label], labels_list))\n",
        "    \n",
        "    #one-hot encoder method\n",
        "    elif method in ['onehot']:\n",
        "        unique_labels = sorted(list(set(labels_list)))\n",
        "        labels_dict = {\n",
        "            label : int(unique_labels.index(label)) for label in unique_labels\n",
        "        }\n",
        "        labels_encoded = list()\n",
        "        for label in labels_list:\n",
        "            label_encoded = len(unique_labels)*[0]\n",
        "            label_number = labels_dict[label]\n",
        "            label_encoded[label_number] = 1\n",
        "            labels_encoded.append(label_encoded)\n",
        "        labels = labels_encoded\n",
        "\n",
        "    #convert list type to array\n",
        "    labels_encoded = np.array(labels)\n",
        "    \n",
        "    if return_categories:\n",
        "        return labels_encoded, labels_dict\n",
        "    else:\n",
        "        return labels_encoded\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define pre-trained words dictionary loader\n",
        "def word_dict_loader(#define file path and file type\n",
        "                     file_path, file_type,\n",
        "                     #define txt and csv file type args\n",
        "                     word_index = None, vector_index = None, header = True, spliter_delimiter = None,\n",
        "                     use_word_spliter = False, word_spliter = None, word_split_index = None,\n",
        "                     #define json file type args\n",
        "                     word_key = None, vector_key = None,\n",
        "                     ):\n",
        "    \n",
        "    word_dict = dict()\n",
        "\n",
        "    #define txt vec loader\n",
        "    if file_type in ['txt', 'vec']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            if header:\n",
        "                next(word_dict_file)\n",
        "            for row in word_dict_file:\n",
        "                row = row.split(spliter_delimiter)\n",
        "                if use_word_spliter:\n",
        "                    word = row[word_index].split(word_spliter)[word_split_index]\n",
        "                else:\n",
        "                    word = row[word_index]\n",
        "                vectors = np.array(row[vector_index:], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "\n",
        "    #define csv loader\n",
        "    elif file_type in ['csv']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            word_dict_file = csv.reader(word_dict_file, delimiter = spliter_delimiter)\n",
        "            if header:\n",
        "                next(word_dict_file)\n",
        "            for row in word_dict_file:\n",
        "                if use_word_spliter:\n",
        "                    word = row[word_index].split(word_spliter)[word_split_index]\n",
        "                else:\n",
        "                    word = row[word_index]\n",
        "                vectors = np.array(row[vector_index:], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "                \n",
        "    #define json loader\n",
        "    elif file_type in ['json']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            word_dict_file = json.load(word_dict_file)\n",
        "            for item in word_dict_file:\n",
        "                word = item[word_key]\n",
        "                vectors = np.array(item[vector_key], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "\n",
        "    #word dict params\n",
        "    word_dict_size = len(word_dict)\n",
        "    word_dict_dim = list(word_dict.values())[0].shape[0]\n",
        "\n",
        "    return word_dict, word_dict_size, word_dict_dim\n"
      ],
      "metadata": {
        "id": "brRWvKPSp4QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define pre-trained embedding word vectors\n",
        "def embd_weights_loader(#define word dictionary and word index\n",
        "                        word_dict, word_index, dimension\n",
        "                        ):\n",
        "    \n",
        "    #create embedding weights\n",
        "    embed_weights = np.zeros([len(word_index)+1, dimension])\n",
        "\n",
        "    for word, index in word_index.items():\n",
        "        if word in word_dict:\n",
        "            embed_weights[index] = word_dict[word]\n",
        "\n",
        "    #embedding layer params\n",
        "    vocab_size = embed_weights.shape[0]\n",
        "    embed_dim = embed_weights.shape[1]\n",
        "\n",
        "    return embed_weights, vocab_size, embed_dim\n"
      ],
      "metadata": {
        "id": "fFr4DSbYpYjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlRqac9W2DWL"
      },
      "outputs": [],
      "source": [
        "#define model\n",
        "def create_model(#define input shape\n",
        "                 input_shape = None,\n",
        "                 #define embedding layer parameters\n",
        "                 use_pretraind_embd = False, vocab_size = None, embd_dim = None,\n",
        "                 sequence_len = None, embed_weights = None,\n",
        "                 #define type of layer and parameters\n",
        "                 use_lstm = False, use_gru = False, use_conv = False,\n",
        "                 #define lstm layers parameters\n",
        "                 lstm_layers_num = None, lstm_layers_units = None,\n",
        "                 #define gru layers parameters\n",
        "                 gru_layers_num = None, gru_layers_units = None,\n",
        "                 #define convolution layers parameters\n",
        "                 conv_layers_num = None, conv_layers_filters = None, conv_layers_kernel = None,\n",
        "                 #define convolution layers sub layers\n",
        "                 use_max_pool = False, max_pool_size = None,\n",
        "                 #define dense layer feeder\n",
        "                 use_global_max_pool = False, use_global_avg_pool = False, use_flatten = False,\n",
        "                 use_feeder_dropout = False, feeder_dropout_ratio = None,\n",
        "                 #define dense head layers\n",
        "                 use_dense_layers = False, dense_layers_num = None, dense_layers_units = None,\n",
        "                 #define dense layers dropout parameters\n",
        "                 use_dense_dropout = False, dense_dropout_ratio = None,\n",
        "                 #define output layer parameters\n",
        "                 output_layer_unit = None, output_layer_activation = None,\n",
        "                 #define model compiler parameters\n",
        "                 optimizer = None, loss = None, metrics = None\n",
        "                 ):\n",
        "    \n",
        "    #define input layer\n",
        "    input = keras.Input(shape = input_shape)\n",
        "\n",
        "    #define embedding layer and parameters\n",
        "    if use_pretraind_embd:\n",
        "        out = keras.layers.Embedding(input_dim = vocab_size, output_dim = embd_dim, input_length = sequence_len,\n",
        "                                     weights = [embed_weights], trainable = False)(input)\n",
        "    else:\n",
        "        out = keras.layers.Embedding(input_dim = vocab_size, output_dim = embd_dim, input_length = sequence_len)(input)\n",
        "\n",
        "    #define type of layer and parameters\n",
        "    #lstm type layers\n",
        "    if use_lstm:\n",
        "        sequence_return = (lstm_layers_num - 1)*[True]\n",
        "        sequence_return.append(False)\n",
        "        for layer_num in range(lstm_layers_num):\n",
        "            out = keras.layers.Bidirectional(keras.layers.LSTM(lstm_layers_units[layer_num],\n",
        "                                                               return_sequences = sequence_return[layer_num]))(out)\n",
        "\n",
        "    #gru type layers\n",
        "    elif use_gru:\n",
        "        sequence_return = (gru_layers_num - 1)*[True]\n",
        "        sequence_return.append(False)\n",
        "        for layer_num in range(gru_layers_num):\n",
        "            out = keras.layers.Bidirectional(keras.layers.GRU(gru_layers_units[layer_num],\n",
        "                                                              return_sequences = sequence_return[layer_num]))(out)\n",
        "    \n",
        "    #convolution type layer\n",
        "    elif use_conv:\n",
        "        for layer_num in range(conv_layers_num):\n",
        "            out = keras.layers.Conv1D(filters = conv_layers_filters[layer_num], kernel_size = conv_layers_kernel[layer_num],\n",
        "                                      activation = 'relu')(out)\n",
        "            if use_max_pool[layer_num]:\n",
        "                out = keras.layers.MaxPool1D(max_pool_size[layer_num])(out)\n",
        "\n",
        "\n",
        "    #dense layers feeder layer\n",
        "    #global max pool type layer\n",
        "    if use_global_max_pool:\n",
        "        out = keras.layers.GlobalMaxPooling1D()(out)\n",
        "        \n",
        "    #global average pool type layer\n",
        "    elif use_global_avg_pool:\n",
        "        out = keras.layers.GlobalAveragePooling1D()(out)\n",
        "\n",
        "    #flatten type layer\n",
        "    elif use_flatten:\n",
        "        out = keras.layers.Flatten()(out)\n",
        "\n",
        "    #define feeder dropout layer\n",
        "    if use_feeder_dropout:\n",
        "        out = keras.layers.Dropout(feeder_dropout_ratio)(out)\n",
        "\n",
        "\n",
        "    #define dense head layers\n",
        "    if use_dense_layers:\n",
        "        for layer_num in range(dense_layers_num):\n",
        "            out = keras.layers.Dense(dense_layers_units[layer_num], activation = 'relu')(out)\n",
        "            if use_dense_dropout[layer_num]:\n",
        "                out = keras.layers.Dropout(dense_dropout_ratio[layer_num])(out)\n",
        "    \n",
        "    #define output layer\n",
        "    output = keras.layers.Dense(output_layer_unit, activation = output_layer_activation)(out)\n",
        "\n",
        "    #define model\n",
        "    model = keras.models.Model(inputs = input, outputs = output)\n",
        "\n",
        "\n",
        "    #compile model\n",
        "    model.compile(optimizer = optimizer,\n",
        "                  loss = loss,\n",
        "                  metrics = metrics)\n",
        "    \n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot model training loss\n",
        "pd.DataFrame(model.history.history)[['loss']].plot(figsize = (9, 6), linewidth = 3)\n",
        "plt.grid(linestyle = '--', linewidth = 2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SsRlkq4i0SVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot model training accuracy\n",
        "pd.DataFrame(model.history.history)[['accuracy']].plot(figsize = (9, 6), linewidth = 3)\n",
        "plt.grid(linestyle = '--', linewidth = 2)\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kkSrgRPU0R7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model evaluation\n",
        "train_set_eval = model.evaluate(texts, labels_encoded, verbose = 0)\n",
        "print(f'Training Set Evaluation:\\n\\tLoss: {round(train_set_eval[0],4)}\\tAccuracy: {100*round(train_set_eval[1],4)}%')\n"
      ],
      "metadata": {
        "id": "z5MKysaj36FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define text generator based on model\n",
        "def text_generator(#define text and numbert of iteration\n",
        "                   input_text, iteration,\n",
        "                   #define tokenizer and maxlen\n",
        "                   tokenizer, maxlen, word_index,\n",
        "                   #define labels categories and model\n",
        "                   labels_dict, model\n",
        "                   ):\n",
        "\n",
        "    input_text = str(input_text)\n",
        "    reversed_labels = dict([(key,value) for value, key in labels_dict.items()])\n",
        "    index_word = dict([(key, value) for value, key in word_index.items()])\n",
        "\n",
        "    for iter in range(iteration):\n",
        "        text = tokenizer.texts_to_sequences([input_text])\n",
        "        text = keras.preprocessing.sequence.pad_sequences(text, padding = 'pre', maxlen = maxlen - 1)\n",
        "        predict = model.predict(text)\n",
        "        predict = np.argmax(predict, axis = -1)[0]\n",
        "        predict = reversed_labels[predict]\n",
        "        predict = index_word[predict]\n",
        "        input_text = input_text + \" \" + predict\n",
        "        \n",
        "    return input_text.title()\n"
      ],
      "metadata": {
        "id": "iCd_e99vP8Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "model.save('/content/model.h5')\n"
      ],
      "metadata": {
        "id": "oKZFO52G0abG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "text-generation-template-notebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGPSe2P72+AhF2B+Kz4/2b",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}