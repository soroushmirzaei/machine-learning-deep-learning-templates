{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soroushmirzaei/projects-notebook-templates/blob/main/text-processing-templates/text-classification-template-notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIYC55FdPdC2"
      },
      "outputs": [],
      "source": [
        "#import requirement libraries\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "#import dataset query libraries\n",
        "import csv\n",
        "import json\n",
        "\n",
        "#import mathematics statics libraries\n",
        "import random as rnd\n",
        "import numpy as np\n",
        "\n",
        "#import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#import machine learning deep learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SkBIh-h6ZzE"
      },
      "outputs": [],
      "source": [
        "#download stop-words dataset\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/english-language-stop-words.txt\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/persian-language-stop-words.txt\n",
        "\n",
        "#download filters-characters dataset\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/english-language-filter-characters.txt\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/persian-language-filter-characters.txt\n",
        "\n",
        "#download similar-characters dataset\n",
        "!wget -q https://raw.githubusercontent.com/soroushmirzaei/text-processing-projects/main/persian-language-similar-characters.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cna6blsV2ZkZ"
      },
      "outputs": [],
      "source": [
        "#define filters-list function loader\n",
        "def filter_chars(file_path):\n",
        "    filter_chars = list()\n",
        "    with open(file_path, 'r') as filters_list_file:\n",
        "        for word in filters_list_file:\n",
        "            filter_chars.append(word.strip('\\n'))\n",
        "        filters_list_file.close()\n",
        "    return filter_chars\n",
        "\n",
        "#define stop-words function loader\n",
        "def stop_word(file_path):\n",
        "    stop_words = list()\n",
        "    with open(file_path, 'r') as stop_words_file:\n",
        "        for word in stop_words_file:\n",
        "            stop_words.append(word.strip('\\n'))\n",
        "        stop_words_file.close()\n",
        "    return stop_words\n",
        "\n",
        "#define similar-characters function loader\n",
        "def similar_chars(file_path):\n",
        "    with open(file_path, 'r') as similar_chars_file:\n",
        "        similar_chars = json.load(similar_chars_file)\n",
        "    return similar_chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLhC44LW6TAd"
      },
      "outputs": [],
      "source": [
        "#load stop-words\n",
        "eng_stop_words = stop_word('english-language-stop-words.txt')\n",
        "per_stop_words = stop_word('persian-language-stop-words.txt')\n",
        "\n",
        "#load filters-characters\n",
        "eng_filter_characters = filter_chars('english-language-filter-characters.txt')\n",
        "per_filter_characters = filter_chars('persian-language-filter-characters.txt')\n",
        "\n",
        "#load similar-characters\n",
        "per_similar_characters = similar_chars('persian-language-similar-characters.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39pSrmpdM5xg"
      },
      "outputs": [],
      "source": [
        "#define remove filters characters function\n",
        "def remove_filter(text, filters_list):\n",
        "    characters = list(text)\n",
        "    characters_without_filters = [character for character in characters if character not in filters_list]\n",
        "    text_without_filters = ''.join(characters_without_filters)\n",
        "    return text_without_filters\n",
        "\n",
        "#define remove texts stopwords function\n",
        "def remove_stopword(text, stop_words_list):\n",
        "    text = text.lower()\n",
        "    words_with_stopwords = text.split(' ')\n",
        "    words_without_stopwords = [word for word in words_with_stopwords if word not in stop_words_list]\n",
        "    text_without_stopwords = ' '.join(words_without_stopwords)\n",
        "    return text_without_stopwords\n",
        "\n",
        "#define similar characters modification function\n",
        "def similar_char(text, similar_chars_dict):\n",
        "    characters = list(text)\n",
        "    similar_characters_modified_list = [similar_chars_dict.get(character,character) for character in characters]\n",
        "    similar_characters_modified_text = ''.join(similar_characters_modified_list)\n",
        "    return similar_characters_modified_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs1sANpkK_xq"
      },
      "outputs": [],
      "source": [
        "#define texts and labels list loader for csv and json files\n",
        "def texts_labels_loader(#define file path and type\n",
        "                        file_path, file_type,\n",
        "                        #define csv and txt files index for text and labels\n",
        "                        text_index = None, label_index = None, header_row = True, spliter_delimiter = None,\n",
        "                        #define json file keys for texts and labels\n",
        "                        text_key = None, label_key = None,\n",
        "                        #define preprocessing function for texts\n",
        "                        use_filter_remover = False, filters_list = None,\n",
        "                        use_stopwords_remover = False, stopwords_list = None,\n",
        "                        use_similarchars_modifier = False, similarchars_dict = None\n",
        "                        ):\n",
        "    \n",
        "    #create empty texts labels list\n",
        "    texts_list = list()\n",
        "    labels_list = list()\n",
        "\n",
        "    #csv file loader\n",
        "    if file_type in ['csv']:\n",
        "        with open(file_path, 'r') as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter = spliter_delimiter)\n",
        "            if header_row:\n",
        "                next(csv_reader)\n",
        "            for row in csv_reader:\n",
        "                text, label = row[text_index], row[label_index]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                if use_stopwords_remover:\n",
        "                    text = remove_stopword(text, stopwords_list)\n",
        "                texts_list.append(text)\n",
        "                labels_list.append(label)\n",
        "        csv_file.close()\n",
        "\n",
        "    #txt file loader\n",
        "    if file_type in ['txt']:\n",
        "        with open(file_path, 'r') as txt_file:\n",
        "            for line in txt_file:\n",
        "                line = line.split(spliter_delimiter)\n",
        "                text, label = line[text_index], line[label_index]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                if use_stopwords_remover:\n",
        "                    text = remove_stopword(text, stopwords_list)\n",
        "                texts_list.append(text.strip('\\n'))\n",
        "                labels_list.append(label.strip('\\n'))\n",
        "        txt_file.close()\n",
        "    \n",
        "    #json file loader\n",
        "    if file_type in ['json']:\n",
        "        with open(file_path, 'r') as json_file:\n",
        "            json_reader = json.load(json_file)\n",
        "            for item in json_reader:\n",
        "                text, label = item[text_key], item[label_key]\n",
        "                #optional modification function\n",
        "                if use_filter_remover:\n",
        "                    text = remove_filter(text, filters_list)\n",
        "                if use_similarchars_modifier:\n",
        "                    text = similar_char(text, similarchars_dict)\n",
        "                if use_stopwords_remover:\n",
        "                    text = remove_stopword(text, stopwords_list)\n",
        "                texts_list.append(text)\n",
        "                labels_list.append(label)\n",
        "        json_file.close()\n",
        "\n",
        "    return texts_list, labels_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzRHgghzLJb6"
      },
      "outputs": [],
      "source": [
        "#define split training validation set\n",
        "def train_valid_split(#define texts labels list\n",
        "                      texts_list, labels_list,\n",
        "                      #define training set size\n",
        "                      train_split_size = None\n",
        "                      ):\n",
        "    \n",
        "    #shuffle texts labels list\n",
        "    texts_labels_zip = zip(texts_list, labels_list)\n",
        "    shuffle_texts_labels_zip = rnd.sample(list(texts_labels_zip), len(texts_list))\n",
        "    texts_list, labels_list = zip(*shuffle_texts_labels_zip)\n",
        "\n",
        "    train_size = int(train_split_size * len(texts_list))\n",
        "\n",
        "    #split train valid set\n",
        "    train_texts_list = texts_list[:train_size]    \n",
        "    train_labels_list = labels_list[:train_size]    \n",
        "\n",
        "    valid_texts_list = texts_list[train_size:]    \n",
        "    valid_labels_list = labels_list[train_size:]    \n",
        "\n",
        "    return train_texts_list, train_labels_list, valid_texts_list, valid_labels_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ3p8PBDbFH2"
      },
      "outputs": [],
      "source": [
        "#define labels encoder\n",
        "def label_encoder(#define labels list and method\n",
        "                  labels_list,\n",
        "                  #define method binary, ordinal or onehot\n",
        "                  method, return_categories = True\n",
        "                  ):\n",
        "    \n",
        "    #ordinal and binary encoder method\n",
        "    if method in ['binary','ordinal']:\n",
        "        unique_labels = sorted(list(set(labels_list)))\n",
        "        labels_dict = {\n",
        "            label : int(unique_labels.index(label)) for label in unique_labels\n",
        "        }\n",
        "        labels = list(map(lambda label : labels_dict[label], labels_list))\n",
        "    \n",
        "    #one-hot encoder method\n",
        "    elif method in ['onehot']:\n",
        "        unique_labels = sorted(list(set(labels_list)))\n",
        "        labels_dict = {\n",
        "            label : int(unique_labels.index(label)) for label in unique_labels\n",
        "        }\n",
        "        labels_encoded = list()\n",
        "        for label in labels_list:\n",
        "            label_encoded = len(unique_labels)*[0]\n",
        "            label_number = labels_dict[label]\n",
        "            label_encoded[label_number] = 1\n",
        "            labels_encoded.append(label_encoded)\n",
        "        labels = labels_encoded\n",
        "\n",
        "    #convert list type to array\n",
        "    labels_encoded = np.array(labels)\n",
        "    \n",
        "    if return_categories:\n",
        "        return labels_encoded, labels_dict\n",
        "    else:\n",
        "        return labels_encoded\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xws6QVTWPwbW"
      },
      "outputs": [],
      "source": [
        "#define tokenizer and sequences and padding sequences\n",
        "def pad_sequences(#define training and validation set\n",
        "                  train_texts, valid_texts,\n",
        "                  #define vocab size and out of vocab word\n",
        "                  vocab_size = None, oov_word = None,\n",
        "                  #define filter characters list\n",
        "                  use_modified_filters = False, filters_list = None,\n",
        "                  #define sequence length, padding and truncating\n",
        "                  sequence_len = None, padding_point = 'post', truncating_point = 'post',\n",
        "                  #define json tokenizer\n",
        "                  save_tokenizer_json = False, tokenizer_filepath = None\n",
        "                  ):\n",
        "    \n",
        "    #define tokenizer and fit\n",
        "    from keras.preprocessing.text import Tokenizer\n",
        "    if use_modified_filters:\n",
        "        filters = ''.join(filters_list)\n",
        "    else:\n",
        "        filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_word,\n",
        "                          filters = filters)\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    if save_tokenizer_json:\n",
        "        with open(tokenizer_filepath+'.json','w') as tokenizer_file:\n",
        "            json.dump(tokenizer.to_json(), tokenizer_file)\n",
        "\n",
        "    #define training validation texts to sequences\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "    valid_sequences = tokenizer.texts_to_sequences(valid_texts)\n",
        "\n",
        "    #define training validation pad sequences\n",
        "    from keras.preprocessing.sequence import pad_sequences\n",
        "    train_padded_sequences = pad_sequences(train_sequences, maxlen = sequence_len,\n",
        "                                           padding = padding_point, truncating = truncating_point)\n",
        "    valid_padded_sequences = pad_sequences(valid_sequences, maxlen = sequence_len,\n",
        "                                           padding = padding_point, truncating = truncating_point)\n",
        "\n",
        "    return train_padded_sequences, valid_padded_sequences, tokenizer, word_index\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define pre-trained words dictionary loader\n",
        "def word_dict_loader(#define file path and file type\n",
        "                     file_path, file_type,\n",
        "                     #define txt and csv file type args\n",
        "                     word_index = None, vector_index = None, header = True, spliter_delimiter = None,\n",
        "                     use_word_spliter = False, word_spliter = None, word_split_index = None,\n",
        "                     #define json file type args\n",
        "                     word_key = None, vector_key = None,\n",
        "                     ):\n",
        "    \n",
        "    word_dict = dict()\n",
        "\n",
        "    #define txt vec loader\n",
        "    if file_type in ['txt', 'vec']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            if header:\n",
        "                next(word_dict_file)\n",
        "            for row in word_dict_file:\n",
        "                row = row.split(spliter_delimiter)\n",
        "                if use_word_spliter:\n",
        "                    word = row[word_index].split(word_spliter)[word_split_index]\n",
        "                else:\n",
        "                    word = row[word_index]\n",
        "                vectors = np.array(row[vector_index:], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "\n",
        "    #define csv loader\n",
        "    elif file_type in ['csv']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            word_dict_file = csv.reader(word_dict_file, delimiter = spliter_delimiter)\n",
        "            if header:\n",
        "                next(word_dict_file)\n",
        "            for row in word_dict_file:\n",
        "                if use_word_spliter:\n",
        "                    word = row[word_index].split(word_spliter)[word_split_index]\n",
        "                else:\n",
        "                    word = row[word_index]\n",
        "                vectors = np.array(row[vector_index:], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "                \n",
        "    #define json loader\n",
        "    elif file_type in ['json']:\n",
        "        with open(file_path, 'r') as word_dict_file:\n",
        "            word_dict_file = json.load(word_dict_file)\n",
        "            for item in word_dict_file:\n",
        "                word = item[word_key]\n",
        "                vectors = np.array(item[vector_key], dtype = 'float32')\n",
        "                word_dict[word] = vectors\n",
        "\n",
        "    #word dict params\n",
        "    word_dict_size = len(word_dict)\n",
        "    word_dict_dim = list(word_dict.values())[0].shape[0]\n",
        "\n",
        "    return word_dict, word_dict_size, word_dict_dim\n"
      ],
      "metadata": {
        "id": "brRWvKPSp4QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define pre-trained embedding word vectors\n",
        "def embd_weights_loader(#define word dictionary and word index\n",
        "                        word_dict, word_index, dimension\n",
        "                        ):\n",
        "    \n",
        "    #create embedding weights\n",
        "    embed_weights = np.zeros([len(word_index)+1, dimension])\n",
        "\n",
        "    for word, index in word_index.items():\n",
        "        if word in word_dict:\n",
        "            embed_weights[index] = word_dict[word]\n",
        "\n",
        "    #embedding layer params\n",
        "    vocab_size = embed_weights.shape[0]\n",
        "    embed_dim = embed_weights.shape[1]\n",
        "\n",
        "    return embed_weights, vocab_size, embed_dim\n"
      ],
      "metadata": {
        "id": "fFr4DSbYpYjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlRqac9W2DWL"
      },
      "outputs": [],
      "source": [
        "#define model\n",
        "def create_model(#define input shape\n",
        "                 input_shape = None,\n",
        "                 #define embedding layer parameters\n",
        "                 use_pretraind_embd = False, vocab_size = None, embd_dim = None,\n",
        "                 sequence_len = None, embed_weights = None,\n",
        "                 #define type of layer and parameters\n",
        "                 use_lstm = False, use_gru = False, use_conv = False,\n",
        "                 #define lstm layers parameters\n",
        "                 lstm_layers_num = None, lstm_layers_units = None,\n",
        "                 #define gru layers parameters\n",
        "                 gru_layers_num = None, gru_layers_units = None,\n",
        "                 #define convolution layers parameters\n",
        "                 conv_layers_num = None, conv_layers_filters = None, conv_layers_kernel = None,\n",
        "                 #define convolution layers sub layers\n",
        "                 use_max_pool = False, max_pool_size = None,\n",
        "                 #define dense layer feeder\n",
        "                 use_global_max_pool = False, use_global_avg_pool = False, use_flatten = False,\n",
        "                 use_feeder_dropout = False, feeder_dropout_ratio = None,\n",
        "                 #define dense head layers\n",
        "                 dense_layers_num = None, dense_layers_units = None,\n",
        "                 #define dense layers dropout parameters\n",
        "                 use_dense_dropout = False, dense_dropout_ratio = None,\n",
        "                 #define output layer parameters\n",
        "                 output_layer_unit = None, output_layer_activation = None,\n",
        "                 #define model compiler parameters\n",
        "                 optimizer = None, loss = None, metrics = None\n",
        "                 ):\n",
        "    \n",
        "    #define input layer\n",
        "    input = keras.Input(shape = input_shape)\n",
        "\n",
        "    #define embedding layer and parameters\n",
        "    if use_pretraind_embd:\n",
        "        out = keras.layers.Embedding(input_dim = vocab_size, output_dim = embd_dim, input_length = sequence_len,\n",
        "                                     weights = [embed_weights], trainable = False)(input)\n",
        "    else:\n",
        "        out = keras.layers.Embedding(input_dim = vocab_size, output_dim = embd_dim, input_length = sequence_len)(input)\n",
        "\n",
        "    #define type of layer and parameters\n",
        "    #lstm type layers\n",
        "    if use_lstm:\n",
        "        sequence_return = (lstm_layers_num - 1)*[True]\n",
        "        sequence_return.append(False)\n",
        "        for layer_num in range(lstm_layers_num):\n",
        "            out = keras.layers.Bidirectional(keras.layers.LSTM(lstm_layers_units[layer_num],\n",
        "                                                               return_sequences = sequence_return[layer_num]))(out)\n",
        "\n",
        "    #gru type layers\n",
        "    elif use_gru:\n",
        "        sequence_return = (gru_layers_num - 1)*[True]\n",
        "        sequence_return.append(False)\n",
        "        for layer_num in range(gru_layers_num):\n",
        "            out = keras.layers.Bidirectional(keras.layers.GRU(gru_layers_units[layer_num],\n",
        "                                                              return_sequences = sequence_return[layer_num]))(out)\n",
        "    \n",
        "    #convolution type layer\n",
        "    elif use_conv:\n",
        "        for layer_num in range(conv_layers_num):\n",
        "            out = keras.layers.Conv1D(filters = conv_layers_filters[layer_num], kernel_size = conv_layers_kernel[layer_num],\n",
        "                                      activation = 'relu')(out)\n",
        "            if use_max_pool[layer_num]:\n",
        "                out = keras.layers.MaxPool1D(max_pool_size[layer_num])(out)\n",
        "\n",
        "\n",
        "    #dense layers feeder layer\n",
        "    #global max pool type layer\n",
        "    if use_global_max_pool:\n",
        "        out = keras.layers.GlobalMaxPooling1D()(out)\n",
        "        \n",
        "    #global average pool type layer\n",
        "    elif use_global_avg_pool:\n",
        "        out = keras.layers.GlobalAveragePooling1D()(out)\n",
        "\n",
        "    #flatten type layer\n",
        "    elif use_flatten:\n",
        "        out = keras.layers.Flatten()(out)\n",
        "\n",
        "    #define feeder dropout layer\n",
        "    if use_feeder_dropout:\n",
        "        out = keras.layers.Dropout(feeder_dropout_ratio)(out)\n",
        "\n",
        "\n",
        "    #define dense head layers\n",
        "    for layer_num in range(dense_layers_num):\n",
        "        out = keras.layers.Dense(dense_layers_units[layer_num], activation = 'relu')(out)\n",
        "        if use_dense_dropout[layer_num]:\n",
        "            out = keras.layers.Dropout(dense_dropout_ratio[layer_num])(out)\n",
        "    \n",
        "    #define output layer\n",
        "    output = keras.layers.Dense(output_layer_unit, activation = output_layer_activation)(out)\n",
        "\n",
        "    #define model\n",
        "    model = keras.models.Model(inputs = input, outputs = output)\n",
        "\n",
        "\n",
        "    #compile model\n",
        "    model.compile(optimizer = optimizer,\n",
        "                  loss = loss,\n",
        "                  metrics = metrics)\n",
        "    \n",
        "    return model\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "text-classification-template-notebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmajxgb4imrGLwvRiLAIUl",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}